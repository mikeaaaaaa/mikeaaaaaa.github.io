---
title: disctribution

date: 2024-11-18 10:40:06

categories: disctribution
---



# 分布式

## 前置知识

### VIP

这里的VIP就是 virtual IP，是实现高可用（HA）的一种方案，高可用本质上就是指通过技术手段避免因为系统出现故障而导致停止对外服务，一般的实现方式是部署提供备用的服务器，在主服务器出现故障的时候接管业务。而VIP就是服务器向客户端提供的一个固定的访问地址；

```java
                                               ______________________
                                              |                      |
                                              | VIP: 193.168.0.6     |
                                        |-----| Host IP: 193.168.0.2 |
                                        |     | Role: Master         |
                                        |     |______________________|
                                        |
                                        |      ______________________
                                        |     |                      |
                                        |     | VIP: Unassigned      |
Public ----(example.com = 193.168.0.6)--|-----| Host IP: 193.168.0.3 |
                                        |     | Role: Slave          |
                                        |     |______________________|
                                        |
                                        |      ______________________
                                        |     |                      |
                                        |     | VIP: Unassigned      |
                                        |-----| Host IP: 193.168.0.4 |
                                              | Role: Slave          |
                                              |______________________|
```

根据上图解释，首先我有三个提供相同服务的服务器，但是对客户端只暴漏一个IP地址 193.168.0.6，这就是 `VIP`，并且这个VIP所对应的实际IP是不固定的，可以进行动态指定；

**原理**

1、Master选举： 集群创建或者Master出现故障时，集群通过选举协议得到一个Master作为对外服务的节点
2、配置VIP： HA软件将VIP配置到Master节点的网卡上
3、ARP广播： 主动对外广播ARP消息，声明VIP对应的MAC地址为Master的网卡MAC地址

## Mysql

关于Mysql的集群不是方案，有非常多，我们挑选几种比较出色的进行介绍。

https://www.cnblogs.com/ricklz/p/17335755.html

### Mysql Replication

Mysql Replication 是`Mysql`官方提供的一种**主从同步方案**，

![mysql](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-b0ac0c4152216059f0dd387e98bf6b39.png)

因为之前有专门学过这套方案，这里就不详细介绍了。

主要流程为：一个主库(M)，三个从库(S)，通过 replication，**Master** 主库生成 event 的 binlog，然后发给 **slave**从库，Slave 将 event 写入 relaylog，然后将其提交到自身数据库中，实现主从数据同步。

1、主库收到更新命令，执行更新操作，生成 binlog;

2、从库在主从之间建立长连接；

3、主库 dump_thread 从本地读取 binlog 传送刚给从库；

4、从库从主库获取到 binlog 后存储到本地，成为 relay log（中继日志）；

5、sql_thread 线程读取 relay log 解析、执行命令更新数据。

优点比较明显：那就是极大增强了我们数据库的**读**d额能力，因为此时每个从库都可以提供查询功能，主库只负责写并且生成数据同步信息；通过增大从服务器的个数，能够极大的增强数据库的读取能力；

缺点就是：数据同步可能存在延时，如果一段时间大量写入操作，可能导致从库落后非常多，另外从库的性能可能相对主库比较弱；另外大量事务的执行也会导致延时；

### semi-sync 半同步复制

MySQL 有三种同步模式，分别是：

1、异步复制：MySQL 中默认的复制是异步的，主库在执行完客户端提交的事务后会立即将结果返回给客户端，并不关心从库是否已经接收并且处理。存在问题就是，如果主库的日志没有及时同步到从库，然后主库宕机了，这时候执行故障转移，在从库冲选主，可能会存在选出的主库中数据不完整；

2、全同步复制：指当主库执行完一个事务，并且等到所有从库也执行完成这个事务的时候，主库在提交事务，并且返回数据给客户端。因为要等待所有从库都同步到主库中的数据才返回数据，所以能够保证主从数据的一致性，但是数据库的性能必然受到影响；

3、半同步复制：是介于全同步和全异步同步的一种，主库至少需要等待一个从库接收并写入到 `Relay Log` 文件即可，主库不需要等待所有从库给主库返回 ACK。主库收到 ACK ，标识这个事务完成，返回数据给客户端。

MySQL 中默认的**复制是异步**的，所以主库和从库的同步会存在一定的延迟，更重要的是异步复制还可能引起数据的丢失。全同步复制的性能又太差了，所以从 `MySQL 5.5` 开始，MySQL 以插件的形式支持 semi-sync 半同步复制。`MySQL 5.7` 引入了增强半同步复制。主库写入数据到 binlog 后，就开始等待从库的应答 ACK，直到至少一个从库写入 `Relay Log` 后，并将数据落盘，然后返回给主库 ACK，通知主库可以进行 commit 操作，然后主库再将事务提交到事务引擎，应用此时才能看到数据的变化。

当客户端发起一个更新事务时，该事务先在本地执行，执行完成之后就要发起对事务的提交操作。在还没有真正提交之前，需要将产生的复制写集广播出去，复制到其它成员。因为事务是通过原子广播发送的，所以组中的成员要么都接收事务，要么都不接收事务。如果组中的所有成员收到了该广播消息(事务)，那么他们会按照之前发送事务的相同顺序收到该广播消息。因此，所有组成员都以相同的顺序接收事务的写集，并为事务建立全局顺序。因此，所有组成员都以相同的顺序接收事务的写集，并为事务建立全局顺序。



### Mysql Group Replication

这项技术由官方于 Mysql 5.7.17版本推出，主要为了解决以往异步复制和半同步复制导致的数据不一致问题；

在技术中定义了由若干节点构成的复制组，一个事务的提交，必须经过组内大多数节点（ N / 2  + 1）决议并通过，才能提交，否则主节点就持续等待；

当客户端发起一个更新事务时，该事务先在本地执行，执行完成之后就要发起对事务的提交操作。在还没有真正提交之前，需要将产生的复制写集广播出去，复制到其它成员。因为事务是通过原子广播发送的，所以组中的成员要么都接收事务，要么都不接收事务。如果组中的所有成员收到了该广播消息(事务)，那么他们会按照之前发送事务的相同顺序收到该广播消息。因此，所有组成员都以相同的顺序接收事务的写集，并为事务建立全局顺序。因此，所有组成员都以相同的顺序接收事务的写集，并为事务建立全局顺序。

![mysql](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-81cd0200f749a27d6dbc91f0ee551d38.jpeg)

![MySQL Group Replication（MGR）-MySQL-Oracle](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRJSDJiC3t-DWupwO-VQrN6RwNsAxvhWieg2A&s)

好处如下：

数据一直性能够得到保障，能保证组内至少一般以上的成员能同步写入数据，

### InnoDB Cluster

最初的 MySQL 版本只提供一种简单的主从异步复制，满足最基本的数据同步。为了提高复制性能，从单线程到组提交再到多线程复制，基本解决了复制延迟问题。为了解决从库与主库的一致性读问题，新增了半同步复制，而为了提供自动故障转移功能，又提供了组复制功能。要做到真正的高可用，失败切换必须对应用透明，于是在组复制的基础上，又发展出了`InnoDB Cluster`。

`InnoDB Cluster` 是官方提供的高可用方案,是 MySQL 的一种高可用性(HA)解决方案，它通过使用 `MySQL Group Replication` 来实现数据的自动复制和高可用性，`InnoDB Cluster` 通常包含下面三个关键组件：

![mysql](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-63eeceaa3458b16988c9b3df0ffbef9f.png)

1、`MySQL Shell`: 它是 MySQL 的高级管理客户端;是 InnoDB Cluster 的管理工具，用于管理和配置集群 (重要)

2、`MySQL Server` 和 `MGR`，使得一组 `MySQL` 实例能够提供高可用性，对于 MGR，`Innodb Cluster` 提供了一种更加易于编程的方式来处理 MGR;

3、`MySQL Router`，一种轻量级中间件，主要进行路由请求，将客户端发送过来的请求路由到不同的 MySQL 服务器节点。是业务流量入口，支持对`MGR`的主从角色判断,可以配置不同的端口分别对外提供读写服务，实现读写分离(重要)

`MySQL Server` 基于 `MySQL Group Replication` 构建，提供自动成员管理，容错，自动故障转移动能等。`InnoDB Cluster` 通常以单主模式运行，一个读写实例和多个只读实例。不过也可以选用多主模式。



InnoDB Cluster 以组复制为基础，集群中的每个 MySQL 服务器实例都是组复制的成员，提供了在 InnoDB Cluster 内复制数据的机制，并且具有内置的故障转移功能。MySQL Shell 在 InnoDB Cluster 中充当控制台角色，使用它包含的`AdminAPI`，可以使安装、配置、管理、维护多个MySQL组复制实例的工作更加轻松。通过`AdminAPI`的几条交互指令就可自动完成组复制配置。MySQL Router 可以根据集群部署信息自动生成配置，将客户端应用程序透明地连接到 MySQL 服务器实例。如果服务器实例意外故障，群集将自动重新配置。在默认的单主模式下，InnoDB Cluster 具有单个读写主服务器实例。多个辅助服务器实例是主服务器实例的副本。如果主服务器出现故障，则辅助服务器将自动升级为主服务器。MySQL Router 可以检测到这种情况并将客户端应用程序自动转发到新的主服务器。





### MMM

MMM（Master-Master replication manager for Mysql）支持双主模式和双主日常管理的脚本程序。可以说是 Mysql 主主复制管理器；

### MHA

### Galera Cluster

### Mysql Cluster

## k8s

### 部署

部署主要是参考这一篇文章：[在Ubuntu22.04 LTS上搭建Kubernetes集群_csdn 136329885-CSDN博客](https://blog.csdn.net/m0_51510236/article/details/136329885)

仅仅靠这一篇文章还不够，还需要结合这篇文章配置代理才行：[Docker、Containerd添加代理_containerd 配置代理-CSDN博客](https://blog.csdn.net/u010589700/article/details/139958293)



### 组件介绍

#### pod

##### 介绍

pod的翻译是 “豌豆荚”，这就非常形象了，豌豆荚里面的一个个小豆子都表示一个容器，即一个pod可以包含一个或者一组容器，他们可以共享**网络或者存储**（我们在docker中是通过创建bridge网络以及映射相同的数据卷来实现通信以及数据共享），而pod天生就可以通信以及数据共享！！！ 

pod是k8s中的最小调度单元；对于一个企业来说一个k8s集群肯定部署非常多个应用程序，因此就有了**逻辑隔离**的需求，引入了**命名空间**，因此我们在查看pod的时候也要注意使用-

n指定命名空间 `kubectl get pod -o wide -n xxx`，其中-o表示显示详细结果；

##### 命令

**创建**：

下面我们讲解如何创建pod：最原始的方法就是通过 `kubectl run nginx --image=nginx:1.19`，创建完后我们可以查看pod的详细信息： `kubectl describe pod nginx`，但是这种方法就是和 `docker run`创建容器一样，不常用，偶尔会用一下；

更加常用的方法是通过`yml`文件来创建：

```yml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
  labels:
    role: mypod
spec:
  containers:
    - name: nginx
      image: nginx:1.7.9
      imagePullPolicy: IfNotPresent
      ports:
        - containerPort: 80
          protocol: TCP
    - name: redis
      image: redis:5.0.10
      imagePullPolicy: IfNotPresent
      ports:
          - containerPort: 6379
  restartPolicy: Always
```



基于yml文件创建pod的命令一共有两个：`apply` 与 `create`：`kubectl create/apply -f xxx.yml `，两个命令的区别在于，create仅仅在不存在时才执行，否则报错，而apply则是不存在则创建，否则更新；

**删除**：

可以根据pod名字或者yml文件删除，因为yml文件中有相应的pod名，所以也是能删除的；

`kubectl delete pod podname`

`kubectl delete -f xxx.yml`

**执行命令**：

kubectl exec -it podname -- 命令

`kubectl exec -it podname -- bash`就是表示进入容器中

但是默认只对pod中的第一个容器起作用，我们可以使用-c来指定具体容器：

`kubectl exec -it podname -c containerName -- bash`

**日志查看**：

`kubectl logs -f (可选，实时显示) nginx(pod名称)` 默认会访问pod中第一个容器的log日志，可以和上述命令一样使用 `-c` 来指定具体的容器

描述信息：

`kubectl describe pod nginx(pod名称)`

**标签**：

标签呢，就是附加到`kubernetes`对象（比如pod）上的键值对，用于指定对用户有意义且相关的对象的标识属性，创建时附加到对象，并可更改和添加，是一组 key-value键值对；就是别名，有了别名就可以**过滤和筛选**；

`kubectl get pods --show-labels`：展示pod的同时展示所有的label标签

`kubectl label --overwrite pod mypod aaa=bbb `：修改特定pod的标签

`kubectl label pod mypod env-`：删除mypod的env标签

但是使用更多的还是通过label进行筛选：

`kubectl get pod -l env=test`

`kubectl get pod -l env`

`kubectl get pod -l '!env'`

`kubectl get pod 'env in (test,prod)'`

`kubectl get pod -l 'env notin (test,pod)'`

##### **生命周期**：

pod遵循预定义的生命周期，起始于 `Pending`阶段，如果至少其中有一个主要容器正常启动，则进入 `Running`，之后取决于`Pod`中是否有容器以失败状态结束而进入 `Succeeded`或者 `Failed`阶段。与此同时 `Pod`在其生命周期中只会被调度一次。一旦Pod被调度到某个节点node，Pod会一直在该节点运行，知道pod被停止；

![image-20241119091510218](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-a2f78551f8d555fabcbc54617812ed14.png)

Pod一旦被创建，会被赋予一个唯一的ID，如果运行该pod的节点死掉了，被调度到该节点 的Pod也会随之删除；Pod本身不具有自愈能力，本质上是K8s具有自愈能力，调度到节点的pod确实会随着node死亡而被删除，Kubernetes使用一种高级抽象来管理这些相对而言可能被随时丢弃的Pod示例，称之为 `控制器`；

任何由ID指定的Pod不会被重新调度，永远只会被一个新的、几乎完全相同的（可以做到仅ID不同）的Pod替换掉（重建），我们要知道其不会共享生命周期就行；如果有个组件生命其生命周期与某个Pod相同，当Pod被删除或者重建时，其也会被删除、重建；

我们可以使用`describe`来查看事件：

![image-20241119092703746](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-addbcbf41332c47769753de34e0476df.png)

#### 容器container

##### 容器的生命周期

一旦pod被调度到某个节点，就开始为pod创建容器，容器有三种状态：Waiting、Running、Terminated；

查看容器状态：`kubectl describe pod mypod(pod名称)`；

+ Waiting：容器仍在完成启动所要做的操作，例如拉去镜像、应用secret数据等，Waiting字段一般会有个Reason字段伴随着解释原因
+ Running：表明容器正在执行并且没有问题发生，如果配置了`postStart`回调，那么该回调已经执行并且完成
+ Terminated：容器正常结束或者因为某些原因结束，如果容器配置了`preStop`回调，则会在进入`Terminated`状态之前执行；

k8s设想的是容器不仅仅要有生命周期，还要有一些钩子函数（生命周期函数），极大的增强了容器的功能；比如说postStart一个最典型的应用就是我们的Java程序，在真正启动前可以在这个函数中检测依赖服务例如mysql、redis是否启动，启动了再执行Java程序；

##### 容器生命周期回调

![image-20241119101109661](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-1a43d792aa2f52537c14e966a2fb6c0a.png)





### 使用技巧

使用kubectl run 命令和docker命令非常相似，可以在命令行直接来创建 pods；但是一般不用一般使用的是 kubectl create 或者 kubectl apply命令来使用`yaml`文件生成；

当我们在新版IDEA中可以非常轻松的创建模板：

![image-20241111215456502](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-a71509b49f79025fe08c5d0e85ed9808.png)

可以看到，只需要数据关键字就可以一键创建，之后使用 kubectl apply xxx.yml即可创建pod示例了！！！更厉害的是什么，我们写好的yaml文件可以使用idea帮助我们一键上传到kube服务器上，只需要创建要给远程主机并进行目录映射即可！！！

![image-20241111220809716](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-6b8806f13544f30d7acc3842aeb79396.png)

删除的话，也可以直接使用创建pod的yml文件删除，因为yml文件中有pod的name；kubectl delete -f xxx.yml

![image-20241111221150350](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-09678d4e2c9f6f502bbb69d688407c54.png)

除了创建，我们也需要进入 pods: `kubectl exec -it <name> --(固定写死) bash(执行命令)`，这种方式只会进入pod的第一个容器，无法进入到其他容器；

如果要进入其他容器：`kubectl exec -it pod名称 -c 容器名称 -- bash`



## Elastic部署

集群部署的docker-compose：

```java
version: "3.4"
services:
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
    container_name: es01
    restart: always
    environment:
      - node.name=es01
      - cluster.name=xblog
      - node.master=true
      - node.data=true
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - http.cors.enabled=true
      - http.cors.allow-origin="*"
      - xpack.security.enabled=true
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.keystore.type=PKCS12
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.keystore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.type=PKCS12
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "TZ=Asia/Shanghai"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
      - ./data/es/es01/data:/usr/share/elasticsearch/data
      - ./data/es/es01/log:/usr/share/elasticsearch/log
      - ./data/es/plugins:/usr/share/elasticsearch/plugins
    networks:
      - net-es
  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
    container_name: es02
    restart: always
    environment:
      - node.name=es02
      - cluster.name=xblog
      - node.master=true
      - node.data=true
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - http.cors.enabled=true
      - http.cors.allow-origin="*"
      - xpack.security.enabled=true
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.keystore.type=PKCS12
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.keystore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.type=PKCS12
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "TZ=Asia/Shanghai"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9201:9200"
      - "9301:9300"
    volumes:
      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
      - ./data/es/es02/data:/usr/share/elasticsearch/data
      - ./data/es/es02/log:/usr/share/elasticsearch/log
      - ./data/es/plugins:/usr/share/elasticsearch/plugins
    networks:
      - net-es
  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
    container_name: es03
    restart: always
    environment:
      - node.name=es03
      - cluster.name=xblog
      - node.master=true
      - node.data=true
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - http.cors.enabled=true
      - http.cors.allow-origin="*"
      - xpack.security.enabled=true
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.keystore.type=PKCS12
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.keystore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.type=PKCS12
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "TZ=Asia/Shanghai"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9202:9200"
      - "9302:9300"
    volumes:
      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
      - ./data/es/es03/data:/usr/share/elasticsearch/data
      - ./data/es/es03/log:/usr/share/elasticsearch/log
      - ./data/es/plugins:/usr/share/elasticsearch/plugins
    networks:
      - net-es
networks:
  net-es:
    driver: bridge

```

启动脚本：

```shell
#!/bin/bash
sudo rm -rf ./data/
mkdir -p ./data/es/{es01,es02,es03}/data
mkdir -p ./data/es/{es01,es02,es03}/log
mkdir -p ./data/es/plugins/elasticsearch-analysis-ik
wget https://ghp.ci/https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.2.0/elasticsearch-analysis-ik-7.2.0.zip -O t.zip
unzip t.zip -d ./data/es/plugins/elasticsearch-analysis-ik
rm -rf t.zip


sudo chmod -R 777 ./data 
sudo chmod -R 777 ./elastic-certificates.p12
sudo docker compose up -d
sleep 10
output=$(sudo docker exec es01 bin/elasticsearch-setup-passwords auto --batch)
echo $output | sed 's/Changed password for user /\n/g' | sed '/^$/d'| tee pwd.txt


```

