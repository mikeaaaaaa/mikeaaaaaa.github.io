---
title: disctribution

date: 2024-11-18 10:40:06

categories: disctribution
---



# 分布式

## 前置知识

### VIP

这里的 VIP 就是 virtual IP，是实现高可用（HA）的一种方案，高可用本质上就是指通过技术手段避免因为系统出现故障而导致停止对外服务，一般的实现方式是部署提供备用的服务器，在主服务器出现故障的时候接管业务。而 VIP 就是服务器向客户端提供的一个固定的访问地址；

```java
                                               ______________________
                                              |                      |
                                              | VIP: 193.168.0.6     |
                                        |-----| Host IP: 193.168.0.2 |
                                        |     | Role: Master         |
                                        |     |______________________|
                                        |
                                        |      ______________________
                                        |     |                      |
                                        |     | VIP: Unassigned      |
Public ----(example.com = 193.168.0.6)--|-----| Host IP: 193.168.0.3 |
                                        |     | Role: Slave          |
                                        |     |______________________|
                                        |
                                        |      ______________________
                                        |     |                      |
                                        |     | VIP: Unassigned      |
                                        |-----| Host IP: 193.168.0.4 |
                                              | Role: Slave          |
                                              |______________________|
```

根据上图解释，首先我有三个提供相同服务的服务器，但是对客户端只暴漏一个 IP 地址 193.168.0.6，这就是 `VIP`，并且这个 VIP 所对应的实际 IP 是不固定的，可以进行动态指定；

**原理**

1、Master 选举： 集群创建或者 Master 出现故障时，集群通过选举协议得到一个 Master 作为对外服务的节点
2、配置 VIP： HA 软件将 VIP 配置到 Master 节点的网卡上
3、ARP 广播： 主动对外广播 ARP 消息，声明 VIP 对应的 MAC 地址为 Master 的网卡 MAC 地址

## Mysql

关于 Mysql 的集群不是方案，有非常多，我们挑选几种比较出色的进行介绍。

https://www.cnblogs.com/ricklz/p/17335755.html

### Mysql Replication

Mysql Replication 是 `Mysql` 官方提供的一种 **主从同步方案**，

![mysql](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-b0ac0c4152216059f0dd387e98bf6b39.png)

因为之前有专门学过这套方案，这里就不详细介绍了。

主要流程为：一个主库(M)，三个从库(S)，通过 replication，**Master** 主库生成 event 的 binlog，然后发给 **slave** 从库，Slave 将 event 写入 relaylog，然后将其提交到自身数据库中，实现主从数据同步。

1、主库收到更新命令，执行更新操作，生成 binlog;

2、从库在主从之间建立长连接；

3、主库 dump_thread 从本地读取 binlog 传送刚给从库；

4、从库从主库获取到 binlog 后存储到本地，成为 relay log（中继日志）；

5、sql_thread 线程读取 relay log 解析、执行命令更新数据。

优点比较明显：那就是极大增强了我们数据库的 **读** d 额能力，因为此时每个从库都可以提供查询功能，主库只负责写并且生成数据同步信息；通过增大从服务器的个数，能够极大的增强数据库的读取能力；

缺点就是：数据同步可能存在延时，如果一段时间大量写入操作，可能导致从库落后非常多，另外从库的性能可能相对主库比较弱；另外大量事务的执行也会导致延时；

### semi-sync 半同步复制

MySQL 有三种同步模式，分别是：

1、异步复制：MySQL 中默认的复制是异步的，主库在执行完客户端提交的事务后会立即将结果返回给客户端，并不关心从库是否已经接收并且处理。存在问题就是，如果主库的日志没有及时同步到从库，然后主库宕机了，这时候执行故障转移，在从库冲选主，可能会存在选出的主库中数据不完整；

2、全同步复制：指当主库执行完一个事务，并且等到所有从库也执行完成这个事务的时候，主库在提交事务，并且返回数据给客户端。因为要等待所有从库都同步到主库中的数据才返回数据，所以能够保证主从数据的一致性，但是数据库的性能必然受到影响；

3、半同步复制：是介于全同步和全异步同步的一种，主库至少需要等待一个从库接收并写入到 `Relay Log` 文件即可，主库不需要等待所有从库给主库返回 ACK。主库收到 ACK ，标识这个事务完成，返回数据给客户端。

MySQL 中默认的 **复制是异步** 的，所以主库和从库的同步会存在一定的延迟，更重要的是异步复制还可能引起数据的丢失。全同步复制的性能又太差了，所以从 `MySQL 5.5` 开始，MySQL 以插件的形式支持 semi-sync 半同步复制。`MySQL 5.7` 引入了增强半同步复制。主库写入数据到 binlog 后，就开始等待从库的应答 ACK，直到至少一个从库写入 `Relay Log` 后，并将数据落盘，然后返回给主库 ACK，通知主库可以进行 commit 操作，然后主库再将事务提交到事务引擎，应用此时才能看到数据的变化。

当客户端发起一个更新事务时，该事务先在本地执行，执行完成之后就要发起对事务的提交操作。在还没有真正提交之前，需要将产生的复制写集广播出去，复制到其它成员。因为事务是通过原子广播发送的，所以组中的成员要么都接收事务，要么都不接收事务。如果组中的所有成员收到了该广播消息(事务)，那么他们会按照之前发送事务的相同顺序收到该广播消息。因此，所有组成员都以相同的顺序接收事务的写集，并为事务建立全局顺序。因此，所有组成员都以相同的顺序接收事务的写集，并为事务建立全局顺序。



### Mysql Group Replication

这项技术由官方于 Mysql 5.7.17 版本推出，主要为了解决以往异步复制和半同步复制导致的数据不一致问题；

在技术中定义了由若干节点构成的复制组，一个事务的提交，必须经过组内大多数节点（ N / 2  + 1）决议并通过，才能提交，否则主节点就持续等待；

当客户端发起一个更新事务时，该事务先在本地执行，执行完成之后就要发起对事务的提交操作。在还没有真正提交之前，需要将产生的复制写集广播出去，复制到其它成员。因为事务是通过原子广播发送的，所以组中的成员要么都接收事务，要么都不接收事务。如果组中的所有成员收到了该广播消息(事务)，那么他们会按照之前发送事务的相同顺序收到该广播消息。因此，所有组成员都以相同的顺序接收事务的写集，并为事务建立全局顺序。因此，所有组成员都以相同的顺序接收事务的写集，并为事务建立全局顺序。

![mysql](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-81cd0200f749a27d6dbc91f0ee551d38.jpeg)

![MySQL Group Replication（MGR）-MySQL-Oracle](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRJSDJiC3t-DWupwO-VQrN6RwNsAxvhWieg2A&s)

好处如下：

数据一直性能够得到保障，能保证组内至少一般以上的成员能同步写入数据，

### InnoDB Cluster

最初的 MySQL 版本只提供一种简单的主从异步复制，满足最基本的数据同步。为了提高复制性能，从单线程到组提交再到多线程复制，基本解决了复制延迟问题。为了解决从库与主库的一致性读问题，新增了半同步复制，而为了提供自动故障转移功能，又提供了组复制功能。要做到真正的高可用，失败切换必须对应用透明，于是在组复制的基础上，又发展出了 `InnoDB Cluster`。

`InnoDB Cluster` 是官方提供的高可用方案, 是 MySQL 的一种高可用性(HA)解决方案，它通过使用 `MySQL Group Replication` 来实现数据的自动复制和高可用性，`InnoDB Cluster` 通常包含下面三个关键组件：

![mysql](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-63eeceaa3458b16988c9b3df0ffbef9f.png)

1、`MySQL Shell`: 它是 MySQL 的高级管理客户端; 是 InnoDB Cluster 的管理工具，用于管理和配置集群 (重要)

2、`MySQL Server` 和 `MGR`，使得一组 `MySQL` 实例能够提供高可用性，对于 MGR，`Innodb Cluster` 提供了一种更加易于编程的方式来处理 MGR;

3、`MySQL Router`，一种轻量级中间件，主要进行路由请求，将客户端发送过来的请求路由到不同的 MySQL 服务器节点。是业务流量入口，支持对 `MGR` 的主从角色判断, 可以配置不同的端口分别对外提供读写服务，实现读写分离(重要)

`MySQL Server` 基于 `MySQL Group Replication` 构建，提供自动成员管理，容错，自动故障转移动能等。`InnoDB Cluster` 通常以单主模式运行，一个读写实例和多个只读实例。不过也可以选用多主模式。



InnoDB Cluster 以组复制为基础，集群中的每个 MySQL 服务器实例都是组复制的成员，提供了在 InnoDB Cluster 内复制数据的机制，并且具有内置的故障转移功能。MySQL Shell 在 InnoDB Cluster 中充当控制台角色，使用它包含的 `AdminAPI`，可以使安装、配置、管理、维护多个 MySQL 组复制实例的工作更加轻松。通过 `AdminAPI` 的几条交互指令就可自动完成组复制配置。MySQL Router 可以根据集群部署信息自动生成配置，将客户端应用程序透明地连接到 MySQL 服务器实例。如果服务器实例意外故障，群集将自动重新配置。在默认的单主模式下，InnoDB Cluster 具有单个读写主服务器实例。多个辅助服务器实例是主服务器实例的副本。如果主服务器出现故障，则辅助服务器将自动升级为主服务器。MySQL Router 可以检测到这种情况并将客户端应用程序自动转发到新的主服务器。





### MMM

MMM（Master-Master replication manager for Mysql）支持双主模式和双主日常管理的脚本程序。可以说是 Mysql 主主复制管理器；

### MHA

### Galera Cluster

### Mysql Cluster

## k8s

### 部署

部署主要是参考这一篇文章：[在 Ubuntu22.04 LTS 上搭建 Kubernetes 集群_csdn 136329885-CSDN 博客](https://blog.csdn.net/m0_51510236/article/details/136329885)

仅仅靠这一篇文章还不够，还需要结合这篇文章配置代理才行：[Docker、Containerd 添加代理_containerd 配置代理-CSDN 博客](https://blog.csdn.net/u010589700/article/details/139958293)



### pod & container

#### pod

##### 介绍

pod 的翻译是 “豌豆荚”，这就非常形象了，豌豆荚里面的一个个小豆子都表示一个容器，即一个 pod 可以包含一个或者一组容器，他们可以共享 **网络或者存储**（我们在 docker 中是通过创建 bridge 网络以及映射相同的数据卷来实现通信以及数据共享），而 pod 天生就可以通信以及数据共享！！！ 

pod 是 k8s 中的最小调度单元；对于一个企业来说一个 k8s 集群肯定部署非常多个应用程序，因此就有了 **逻辑隔离** 的需求，引入了 **命名空间**，因此我们在查看 pod 的时候也要注意使用-

n 指定命名空间 `kubectl get pod -o wide -n xxx`，其中-o 表示显示详细结果；

##### 命令

**创建**：

下面我们讲解如何创建 pod：最原始的方法就是通过 `kubectl run nginx --image=nginx:1.19`，创建完后我们可以查看 pod 的详细信息： `kubectl describe pod nginx`，但是这种方法就是和 `docker run` 创建容器一样，不常用，偶尔会用一下；

更加常用的方法是通过 `yml` 文件来创建：

```yml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
  labels:
    role: mypod
spec:
  containers:
    - name: nginx
      image: nginx:1.7.9
      imagePullPolicy: IfNotPresent
      ports:
        - containerPort: 80
          protocol: TCP
    - name: redis
      image: redis:5.0.10
      imagePullPolicy: IfNotPresent
      ports:
          - containerPort: 6379
  restartPolicy: Always
```



基于 yml 文件创建 pod 的命令一共有两个：`apply` 与 `create`：`kubectl create/apply -f xxx.yml `，两个命令的区别在于，create 仅仅在不存在时才执行，否则报错，而 apply 则是不存在则创建，否则更新；

**删除**：

可以根据 pod 名字或者 yml 文件删除，因为 yml 文件中有相应的 pod 名，所以也是能删除的；

`kubectl delete pod podname`

`kubectl delete -f xxx.yml`

**执行命令**：

kubectl exec -it podname -- 命令

`kubectl exec -it podname -- bash` 就是表示进入容器中

但是默认只对 pod 中的第一个容器起作用，我们可以使用-c 来指定具体容器：

`kubectl exec -it podname -c containerName -- bash`

**日志查看**：

`kubectl logs -f (可选，实时显示) nginx(pod名称)` 默认会访问 pod 中第一个容器的 log 日志，可以和上述命令一样使用 `-c` 来指定具体的容器

描述信息：

`kubectl describe pod nginx(pod名称)`

**标签**：

标签呢，就是附加到 `kubernetes` 对象（比如 pod）上的键值对，用于指定对用户有意义且相关的对象的标识属性，创建时附加到对象，并可更改和添加，是一组 key-value 键值对；就是别名，有了别名就可以 **过滤和筛选**；

`kubectl get pods --show-labels`：展示 pod 的同时展示所有的 label 标签

`kubectl label --overwrite pod mypod aaa=bbb `：修改特定 pod 的标签

`kubectl label pod mypod env-`：删除 mypod 的 env 标签

但是使用更多的还是通过 label 进行筛选：

`kubectl get pod -l env=test`

`kubectl get pod -l env`

`kubectl get pod -l '!env'`

`kubectl get pod 'env in (test,prod)'`

`kubectl get pod -l 'env notin (test,pod)'`

##### **生命周期**：

pod 遵循预定义的生命周期，起始于 `Pending` 阶段，如果至少其中有一个主要容器正常启动，则进入 `Running`，之后取决于 `Pod` 中是否有容器以失败状态结束而进入 `Succeeded` 或者 `Failed` 阶段。与此同时 `Pod` 在其生命周期中只会被调度一次。一旦 Pod 被调度到某个节点 node，Pod 会一直在该节点运行，知道 pod 被停止；

![image-20241119091510218](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-a2f78551f8d555fabcbc54617812ed14.png)

Pod 一旦被创建，会被赋予一个唯一的 ID，如果运行该 pod 的节点死掉了，被调度到该节点 的 Pod 也会随之删除；Pod 本身不具有自愈能力，本质上是 K8s 具有自愈能力，调度到节点的 pod 确实会随着 node 死亡而被删除，Kubernetes 使用一种高级抽象来管理这些相对而言可能被随时丢弃的 Pod 示例，称之为 `控制器`；

任何由 ID 指定的 Pod 不会被重新调度，永远只会被一个新的、几乎完全相同的（可以做到仅 ID 不同）的 Pod 替换掉（重建），我们要知道其不会共享生命周期就行；如果有个组件生命其生命周期与某个 Pod 相同，当 Pod 被删除或者重建时，其也会被删除、重建；

我们可以使用 `describe` 来查看事件：

![image-20241119092703746](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-addbcbf41332c47769753de34e0476df.png)

#### 容器 container

##### 容器的生命周期

一旦 pod 被调度到某个节点，就开始为 pod 创建容器，容器有三种状态：Waiting、Running、Terminated；

查看容器状态：`kubectl describe pod mypod(pod名称)`；

+ Waiting：容器仍在完成启动所要做的操作，例如拉去镜像、应用 secret 数据等，Waiting 字段一般会有个 Reason 字段伴随着解释原因
+ Running：表明容器正在执行并且没有问题发生，如果配置了 `postStart` 回调，那么该回调已经执行并且完成
+ Terminated：容器正常结束或者因为某些原因结束，如果容器配置了 `preStop` 回调，则会在进入 `Terminated` 状态之前执行；

k8s 设想的是容器不仅仅要有生命周期，还要有一些钩子函数（生命周期函数），极大的增强了容器的功能；比如说 postStart 一个最典型的应用就是我们的 Java 程序，在真正启动前可以在这个函数中检测依赖服务例如 mysql、redis 是否启动，启动了再执行 Java 程序；

##### 容器生命周期回调

![image-20241119101109661](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-1a43d792aa2f52537c14e966a2fb6c0a.png)

##### 容器的重启策略

其实是无法单独对 `container` 进行重启策略配置的，k8s 仅支持对整个 pod 中的所有容器配置重启策略；重启策略有三种：Always（总是重启）、OnFailure（非正常退出则重启）、Never 三种，默认为 Always；

有人可能会疑惑 Always 和 OnFailure 的区别是什么：

![image-20241124210918240](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-6676baf9522df06d650a84dff50725ea.png)

而 OnFailure 在这种情况下不会重启；

##### 自定义容器启动命令

 和 docekr 容器一样，k8s pod 中的容器也支持通过 command、args 来修改容器启动默认执行命令以及传递相关参数；

通过名称也可以推断出，args 是用来传参的，command 是用来修改默认命令的：

![image-20241124211805773](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-b0ca2ecb9c840585ee2187d650a57737.png)

当然我们也可以只使用 `args`，即命令也通过 `args` 来传参；

##### 容器探针

我们可以为每个容器定义各种探针，用来探测容器做出什么反应，我们使用的探针主要分以下三种：

+ `livenessProb`：指示容器是否运行，若检测失败，kubectl 会杀死容器，并执行容器重启策略，不提供该探针默认为 success
+ `readinessProbs`：指示容器是否准备好提供服务。如果如果探测失败，端点控制器则将其从服务列表中删除它的 IP 地址，
+ `startupProbe`：指示容器中应用是否启动

探诊结果分为 Success、Failure、Unknown； 

每个探针具体的执行机制可以从以下四种来选择：

+ exec：在容器中执行某些命令，如果命令退出返回 0 则认为诊断成功
+ grpc：使用 gRPC 执行一个远程调用。目标应该事先 gRPC 健康检查，响应状态为 Serving，则诊断成功
+ httpGet：对容器 IP 地址上指定的端口和路径发送 GET 请求，相应状态码非 5xx、4xx 则成功
+ tcpSocket：xxxx



基本操作：

![image-20241124213747671](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-e1d0c99d059b1ba72d96a4eab43ff95f.png)

如果通过 Http get 方式的探针，定义也非常简单：

![image-20241124214709215](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-12162650d1c72e4ef8b74ff83b059b73.png)

##### 容器的资源限制

如果不对资源限制，容器是能够无限制使用当前节点的 cpu、内存资源；通过 request memory 和 request cpu 来保障容器有哦那个有它请求的内存和 cpu 资源，通过 limit memory 和 limit cpu 来限制容器可使用最大的内存、cpu 资源；（就是 **基础资源** 和 **最大资源**）；

具体讲解前我们先了解一下 Metrics Server 服务器，他是一个可扩展的、高效的容器资源度量源 。它可以监控每一个 pod、container 的负载，Metrics Server 从 Kubelets 收集资源指标，并自己提供 API 供 Horizontal Pod、Vertical  Pod Autoscaler 使用。Metrics API 也可以通过 kubectl top 命令来访问；初始是没有这个服务器的，我们需要通过 `yml` 文件来 `kubectl apply -f metrics-server.yml` 创建这个服务器;

安装完后使用：`kubetl top pod mypod(pod名称)` 即可查看容器资源；



显示内存、cpu 的目的：![image-20241125134649521](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-11333fadc20acc787d50a41646a2ad03.png)

#### init容器

init容器是一种特殊的容器，它可以在pod中所有的应用容器启动之前运行，Init容器可以包含一些应用镜像中不存在的**实用工具**和**安装脚本**。

Init容器可以定义多个，但是需要按顺序启动。而且Init容器必须启动成功才会去尝试启动应用容器；

Init容器仅仅不支持生命周期、不支持探针，其他基本和正常容器相同； 

下面尝试建立一个简单的BusyBox的例子，由于BusyBox不运行任何服务会导致挂掉，因此我们得执行sleep指令；

![image-20241125200219693](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-d069db01e02d85f3a0fd73dd8db3ee04.png)

#### 节点亲和性分配 Pod

实际上我们是可以显示的指定Pod运行在哪个节点上的，或者优先在特定的节点上运行，有几种方法可以实现这点，推荐的方法是**标签选择符进行选择**；

假如我们一个pod提供web应用，另一个pod不断对web应用进行日志采集，像这种有密切联系、大量通信的pod节点之间需要放置到同一个节点上或者距离尽可能近；

具体分配方式：

+ 与节点标签匹配的nodeSelector（给节点打标签，在创建pod时指定标签）
+ 亲和性与反亲和性
+ nodeName字段
+ Pod拓扑分布约束

（1）标签匹配

我们可以使用 `kubectl get ndoes --show-labels`来查看节点当前的标签

也可以使用 `kubectl label nodes k8s-node1(节点名称) cpu=fast`来给节点增加标签；

![image-20241125220344618](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-d4ac4607e4b42ecb4c51b204e2ab4de1.png)

（2）使用节点名字来指定节点

![image-20241125220621497](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-cc446b11992144b7e048616c106386de.png)

（3）根据亲和性和反亲和性

都有了标签、节点名匹配为什么还需要亲和性呢？因为以上配置的要求都是硬性的，就是不满足不好意思，你就部署不了；而亲和性就更加人性化；

亲和性主要分为两种：`节点亲和性`、`pod间亲和性`；这两种都能够指派pod的分配；

​	节点亲和性概念上类似`nodeSelector`，它可以使你根据节点上的标签来约束Pod可以调度到哪些节点上；具体来说节点亲和性又分为强制（requiredDuringSchedulingIgnoreDuringEception）和非强制（preferredDuringSchedulingIgnoredDuringException）：

![image-20241125222954486](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-b738f35be33ef7a243753a9eee3b7eb0.png)

​	pod亲和性：pod亲和性就非常适用于通信非常紧密的pod之间；与节点亲和性一样，这里也分为强制（requiredDuringSchedulingIgnoreDuringEception）和非强制（preferredDuringSchedulingIgnoredDuringException）两种子类型；使用的字段为 `spec.affinity.podAffinity`,并且使用到了pod的`label`标签，并且引入了 `域`的概念，域实际上是用来聚集node节点的，不同的node是可以归结到同一个域中，而pod亲和性首先是选择某个域，其次再选择label进行域中的pod选择：

![image-20241126113332337](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-d07b6a15db2d7b6cad3a9939b8eeccf4.png)

### Controller控制器

#### 控制器介绍

在k8s中我们通常不会像3.2的介绍那样单独的创建pod，通常是通过Controller来管理pod，来帮助我们将pod当前状态转化为我们期望的状态；比如在`controller`中我么你可以定义pod的部署特性，比如有几个副本、在什么样的Node上运行，通俗的说，可以认为Controller就是用来**管理Pod的一个对象**，其核心作用：**<u>通过监控集群的公共状态，并致力于将当前状态转变为期望的状态</u>**；比如我在服务高峰期时想要拓展pod的个数；

#### 创建控制器

+ `Deployment`：是最常用的控制器，可以管理Pod的多个副本，并确保Pod按照期望的状态来运行
    + ReplicaSet 实现了Pod的多副本管理。使用Deplyment时会自动创建ReplicaSet，也就是说Deployment是通过ReplicaSet来管理Pod的多副本，我们通常不需要直接使用ReplicaSet
+ `Daemonset`：用于每个Node最多只运行一个Pod副本的场景，正如其名称所揭示的，DaemonSet通常运行daemon
+ `StateFulSet`：目前来说和Deployment非常类似，特点是 能够保证Pod的每个副本的名称在整个生命周期中都是不变的，而其他的Controller不能保证，当某个Pod发生故障需要重启时，StatufulSet副本会按照固定的顺序启动、更新、删除
+ `Job`：用于运行运行完就结束的pod，其他Controller中的Pod通常是长期运行的

#### Controller 管理Pod

> 注意，Controller通过label关联起来Pods

![image-20241126122325835](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-be034dbd23479b878e9346c7c2ca2cc5.png)

上图中我们可以看到，Deployment管理一组使用同一标签的pod；我们在创建控制器时需要制定控制器管理含有哪些标签的pod；

#### Deployment

![image-20241126124807774](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-8210bde88dd065596bfe9770bdef6a14.png)

![image-20241126130500710](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-41fb370d6d762bce3a48d4860f45624b.png)

**命令**：

+ `kubectl apply -f xxx.yml` 创建
+ `kubectl get deployments`查看所有的deployment控制器
+ `kubectl get deployment --show-labels `来查看所有的deployment的标签
+ `kubectl get pod -o wide` 查看pod详情
+ `kubectl logs -f pod-name`：查看pod日志
+ `kubectl exec -it pod-name -c container-name -- bash`
+ `kubectl get deployment nginx-deployment -o yaml`：展示deployment控制器更加详细的信息
+ `kubectl delete -f xxx.yaml`：根据文件删除deployment

Pod**扩缩命令**：

+ `kubectl  get replicaset（简写 kubectl get rs）`：用来程序副本
+ `kubectl scale deployment nginx(deploymentName) --replicas=3`：扩展成三个pod节点，拓展的pod会根据定义的分配规则来决定被分到哪个节点上去；

Pod**版本控制**命令：

+ `kubectl rollout status [deployment nginx-deployment | deployment/nginx-deployment]`：查看状态
+ `kubectl rollout history deployment nginx-deployment`：查看历史
+ `kubectl rollout history deployment/nginx-deployment --revison=2`：查看某次历史版本的详细信息
+ `kubectl rollout undo depolyment nginx-deployment`：回到上一个版本
+ `kubectl rollout undo deployment nginx-deployment --to-revision=2`：回到指定版本
+ `kubectl rollout restart depolyment nginx-deployment`：重新部署
+ `kubectl rollout pause/resume depolyment nginx-deployment`：暂停/恢复应用

删除命令：

kubectl delete -f xxx.yml



#### StatefulSet

**无状态应用**：应用本身不需要存储任何数据的应用；**有状态应用**：需要存储数据的应用；例如Java前端、后端都是无状态应用，而Mysql、redis、es等都属于有状态应用；

而StatefulSet就是专门用于管理有状态应用的工作负载API对象，还用来管理某`pod`集合的部署和扩缩（和deployment一样提供扩缩）并为这些pod**提供持久的存储和持久的标识符**：使得该控制器创建的每一个`Pod`副本都能够动态的创建“存储卷”，这个k8s中的数据卷不单独属于某个节点，类似于一个专门提供存储服务的第三方发，之后统一在这样一个第三方持久化数据，同时当`pod`重建的时候，重建的pod拥有相同的id，这样即使pod重启，持久化的数据也能够重新读取，而`deployment`不支持持久的存储并且每次重建pod id均会改变，持久化存储标识符每次都不一样；关键在于这个**粘性ID**

![image-20241201135751876](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-12-7fa8e0ba5782298879bb06ab74849f68.png)

当我们有以下需求是我们可以选择使用StatufulSet标识符：

1. 稳定唯一的网络标识符：不仅可以保持唯一的ID，还可以保持一个**唯一的网络标识符**
2. 稳定持久的存储：上面说过了
3. 有序的优雅的部署和扩缩：假如有多个副本的话，StatufulSet能够运行定制各个启动的顺序；删除也是可以有序的
4. 有序自动的滚动更新

一般的话，例如mysql等有状态应用我们都是单独部署的，因此我们使用即使这个Controlle非常强大，我们也不常使用，使用更多的还是Deployment；





由于演示要用到持久化存储，我们需要首先搭建一个 NFS（network file system），这个网络文件系统是独立于k8s,下面演示创建nfs流程：

1. 单独起一个Ubuntu系统，系统安装部署nfs服务，https://bbs.huaweicloud.com/blogs/399865
2. 对每一个node安装nfs客户端，创建本地目录、挂载远程nfs目录到本地（mount -t nfs ip:/root/nfs /root/nfs），挂在完后就可以实现网络数据同步了
3. 但是我们也要在pod集群中创建一个专门用来管理NFS的pod，用于提供服务，我们直接在github上使用人家创建好的 yml文件即可创建：`nfs-client-provisioner`



#### Service

##### Service介绍

通俗定义：将pod**提供网络服务**的一种方式，让pod具有被网络访问的能力；

假如说现在有一个先后端应用程序，前端需要调用后端的API：

![image-20241201155822755](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-12-77b5965bf0b400ee0a9aa770f98397cc.png)

那我得告诉你在哪啊，对于前端老说，需要配置每一个后端，但当我们引入了Service时，前端就根本不需要知道他们调用了哪个后端副本，极大的解耦了，并且还具有负载均衡的能力：

![image-20241201160024946](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-12-936013d87d0f46a4864b31806ef77c77.png)

注意：Service是pod与pod之间的，如果时内部与外部之间，那么则还需要其他的组件；



+ service和Controller一样，也是通过`label`与其他容器关联，即通过label管理pod
+ Service的生命周期不与Pod绑定，不会因为Pod重新创建而改变IP
+ 提供了负载均衡功能，自动转发流量到不同的Pod
+ 集群内布可以通过服务名称访问
+ 可对集群外部提供访问端口

##### 使用

  一般我们Service和Deployment写在同一个yml文件中，因为service是另一种对象，因此在yml文件中写的时候呢就得使用 `---`分隔

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3  # 设置副本数
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest  # 使用最新的Nginx镜像
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx  # 匹配Label，服务将路由到带有app: nginx标签的Pod
  ports:
    - protocol: TCP
      port: 80  # 服务暴露的端口
      targetPort: 80  # 转发到Pod的端口
  type: LoadBalancer  # 如果在云环境中运行，可以使用LoadBalancer类型暴露服务

```

这里我们就得提及一下k8s中的命名空间

### 使用技巧

使用 kubectl run 命令和 docker 命令非常相似，可以在命令行直接来创建 pods；但是一般不用一般使用的是 kubectl create 或者 kubectl apply 命令来使用 `yaml` 文件生成；

当我们在新版 IDEA 中可以非常轻松的创建模板：

![image-20241111215456502](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-a71509b49f79025fe08c5d0e85ed9808.png)

可以看到，只需要数据关键字就可以一键创建，之后使用 kubectl apply xxx.yml 即可创建 pod 示例了！！！更厉害的是什么，我们写好的 yaml 文件可以使用 idea 帮助我们一键上传到 kube 服务器上，只需要创建要给远程主机并进行目录映射即可！！！

![image-20241111220809716](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-6b8806f13544f30d7acc3842aeb79396.png)

删除的话，也可以直接使用创建 pod 的 yml 文件删除，因为 yml 文件中有 pod 的 name；kubectl delete -f xxx.yml

![image-20241111221150350](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2024-11-09678d4e2c9f6f502bbb69d688407c54.png)

除了创建，我们也需要进入 pods: `kubectl exec -it <name> --(固定写死) bash(执行命令)`，这种方式只会进入 pod 的第一个容器，无法进入到其他容器；

如果要进入其他容器：`kubectl exec -it pod名称 -c 容器名称 -- bash`



## Elastic 部署

集群部署的 docker-compose：

```java
version: "3.4"
services:
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
    container_name: es01
    restart: always
    environment:
      - node.name=es01
      - cluster.name=xblog
      - node.master=true
      - node.data=true
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - http.cors.enabled=true
      - http.cors.allow-origin="*"
      - xpack.security.enabled=true
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.keystore.type=PKCS12
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.keystore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.type=PKCS12
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "TZ=Asia/Shanghai"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
      - ./data/es/es01/data:/usr/share/elasticsearch/data
      - ./data/es/es01/log:/usr/share/elasticsearch/log
      - ./data/es/plugins:/usr/share/elasticsearch/plugins
    networks:
      - net-es
  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
    container_name: es02
    restart: always
    environment:
      - node.name=es02
      - cluster.name=xblog
      - node.master=true
      - node.data=true
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - http.cors.enabled=true
      - http.cors.allow-origin="*"
      - xpack.security.enabled=true
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.keystore.type=PKCS12
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.keystore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.type=PKCS12
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "TZ=Asia/Shanghai"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9201:9200"
      - "9301:9300"
    volumes:
      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
      - ./data/es/es02/data:/usr/share/elasticsearch/data
      - ./data/es/es02/log:/usr/share/elasticsearch/log
      - ./data/es/plugins:/usr/share/elasticsearch/plugins
    networks:
      - net-es
  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
    container_name: es03
    restart: always
    environment:
      - node.name=es03
      - cluster.name=xblog
      - node.master=true
      - node.data=true
      - discovery.seed_hosts=es01,es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - http.cors.enabled=true
      - http.cors.allow-origin="*"
      - xpack.security.enabled=true
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.keystore.type=PKCS12
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.keystore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.path=/usr/share/elasticsearch/config/elastic-certificates.p12
      - xpack.security.transport.ssl.truststore.type=PKCS12
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "TZ=Asia/Shanghai"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9202:9200"
      - "9302:9300"
    volumes:
      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12
      - ./data/es/es03/data:/usr/share/elasticsearch/data
      - ./data/es/es03/log:/usr/share/elasticsearch/log
      - ./data/es/plugins:/usr/share/elasticsearch/plugins
    networks:
      - net-es
networks:
  net-es:
    driver: bridge

```

启动脚本：

```shell
#!/bin/bash
sudo rm -rf ./data/
mkdir -p ./data/es/{es01,es02,es03}/data
mkdir -p ./data/es/{es01,es02,es03}/log
mkdir -p ./data/es/plugins/elasticsearch-analysis-ik
wget https://ghp.ci/https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.2.0/elasticsearch-analysis-ik-7.2.0.zip -O t.zip
unzip t.zip -d ./data/es/plugins/elasticsearch-analysis-ik
rm -rf t.zip


sudo chmod -R 777 ./data 
sudo chmod -R 777 ./elastic-certificates.p12
sudo docker compose up -d
sleep 10
output=$(sudo docker exec es01 bin/elasticsearch-setup-passwords auto --batch)
echo $output | sed 's/Changed password for user /\n/g' | sed '/^$/d'| tee pwd.txt


```

## NFS

