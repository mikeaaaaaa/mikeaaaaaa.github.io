# 爬虫

## 介绍

之前学习的爬虫都是请求网页，之后使用bs4或者xpath、正则表达式来对网页进行解析，获取到我们想要的数据；

但是，这种太慢了，对于大批量的数据爬取是不行的，更直接的，对于现如今更常用的动态网站，其前段展示的数据肯定是通过向后端的API请求获取的，因此，我们只需要找到后端请求的API，直接请求获取数据是更加方便的方法；

我们还需要了解一下使用浏览器访问一个网页和使用`requests`库访问一个网页之间的区别：

+ `requests` 库仅仅发送 HTTP 请求并获取响应，它不会像浏览器那样渲染 HTML、加载 JavaScript、执行动态脚本、处理 CSS、生成 DOM 树等。它**只会获取 HTML 内容**，基本上不会处理页面中依赖 JavaScript 的动态内容（例如，通过 AJAX 请求动态加载的数据）。
+ `Selenium` 是一个非常强大的工具，可以模拟浏览器操作，包括渲染动态网页、执行 JavaScript 和处理异步加载的内容。使用 `Selenium`，你可以启动一个浏览器（如 Chrome 或 Firefox），模拟用户的浏览行为，**获取渲染后的网页内容**。

使用非常广泛的爬虫框架Scrapy默认使用的就是使用`requests`库进行的访问，但是我们也可以通过在`settings.py`中配置自定义的`DOWNLOADER_MIDDLEWARES`下载中间件，来自定义请求方式；

```java
class NewsDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        if config.DOWNLOADER_MIDDLEWARE_USE_PROXY:
            proxy = config.DOWNLOADER_PROXY_URL
            spider.logger.info("Use global proxy: {}, spider name: {}".format(proxy, spider.name))
            request.meta["proxy"] = proxy

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called

        if spider.name in selenium_list:
            option = webdriver.ChromeOptions()
            option.add_argument('--headless')  # 无界面运行
            option.add_argument('--disable-gpu')  # 禁止gpu加速
            option.add_argument("no-sandbox")  # 取消沙盒模式
            option.add_argument("disable-blink-features=AutomationControlled")  # 禁用启用Blink运行时的功能
            option.add_experimental_option('excludeSwitches', ['enable-automation'])  # 开发者模式
            driver = webdriver.Chrome(options=option)

            # 移除 `window.navigator.webdriver`. scrapy 默认为True
            driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
                "source": """
                    Object.defineProperty(navigator, 'webdriver', {
                      get: () =&gt; undefined
                    })
                  """
            })
            driver.get(request.url)
            driver.implicitly_wait(5)
            content = driver.page_source
            driver.quit()

            return HtmlResponse(request.url, encoding="utf-8", body=content, request=request)

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.
        if isinstance(exception, (TimeoutError, TCPTimedOutError)):
            spider.logger.error(f"Request timeout: {request.url}")
            return request
        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info("Spider opened: %s" % spider.name)
        spider.logger.info("datetime_start: {}, datetime_end: {}".format(spider.datetime_start, spider.datetime_end))
        spider.logger.info("task_id: {}".format(spider.task_id))
```



## 环境配置

安装 `google浏览器` + `Google driver` + `selenium3.141.0`

关于google driver的版本：

![image-20250103160745992](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2025-01-ff14688b759ec02bcb24b280bc24dc4b.png)

安装方法：选择一个版本最接近的`google driver`压缩包，下载网址https://developer.chrome.com/docs/chromedriver?hl=zh-cn，使用json节点的方式获取下载链接，解压后将*.exe文件复制到python.exe文件夹中：

![image-20250103161805537](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2025-01-650cfd6f0415ab09b3c02fe5b7d0b863.png)

安装 `selenium`：

> pip install selenium

测试用例：

```python
from selenium import webdriver
webdriver.Chrome()

input("按任意键退出")
```

### Scrapy使用

这是一个爬虫框架：

![img](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2025-01-26fc3dd30bf764fbacdec1502c9bd96c.png)

1、Spider：这是Scrapy的核心概念，负责定义**如何抓取一个网站**，**解析网站上的内容**，并**提取数据**。你需要编写 Spider 来告诉 Scrapy 如何抓取一个或多个页面

2、item：用來表示你从网页上**抓取的内容**，类似于字典，但是提供了额外的功能，如字段验证

3、Pipeline：用来处理抓取到的数据，它允许你**清洗、验证、持久化等**，可以定义多个处理步骤

4、Selector：Scrapy使用**XPATH和CSS选择器**来从网页中提取数据，Selector是Scrapy提供的一个对象，用于通过这两种方式从网页中提取数据

## 爬虫实战

首先了解一下爬虫能干嘛：

1、定制化的数据采集

2、自动点赞、评论、刷票、商品抢购脚本、自动发送弹幕

其次，明确需求：

1、采集的网站是什么

2、采集的数据分析

再其次，数据解析：

1、正则表达式

2、html节点

3、xpath



### 小说网站

网址：https://734a21c0fe5c8d6884.bq03.cc/read/45394/1.html



原理啥的也不多讲吧，那就稍微讲一下组里使用的爬虫框架的使用：

![image-20250105124215737](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2025-01-e08374653f99b6c4ca0b9716eb125e27.png)

有`database`目录是因为我使用到了mysql，在爬虫搜集到数据之后立即存储到数据库中；但是使用和我说，在`Scrapy`中，数据更多的是直接存储近MongoDB或者redis中，之后数据更加精细化处理是交给其他服务来做的；

news是项目最终可以导出的 `库`，config.settings下的config使用`pydantic_settings`的`BaseSetggings`类来存储用户的个性化配置，例如mysql的ip、端口等；然后Scrapy自带的`settings.py`文件也能够存储配置信息，但这里存放的信息一般我们不再动，数据是框架的配置信息；

最重要的包就是我么你的`spiders`，里面就是写了我们的爬虫代码，主要实现一个 `parse`函数，挺简单的，看看师兄写好的代码照着写就行，目的是讲爬取到的数据封装成对象交给`pipeline`进一步处理； `items.py`就是爬取信息类；



说白了关键在于编写具体的爬虫类；另外还学习到了写爬虫的好办法，为了`调试`，我們需要编写一个这个类然后debug执行：

```python
from scrapy import cmdline

cmdline.execute(['scrapy', 'crawl', 'xjtu',
                 '-a', 'task_id=123',
                 '-a', 'datetime_start=2024-07-19 00:00:14',
                 '-a', 'datetime_end=2024-07-21 23:36:44'])
```

下断点之后`停下来`，有一个console，**在这里我们可以 动态的写代码**，这对于写爬虫来说真的非常方便；

下**图中左边是查看当前断点处变量信息的，右边是交互式的命令行；**

![image-20250105125557265](https://raw.githubusercontent.com/mikeaaaaaa/cloudimg/main/img/2025-01-125ea22d4707a1723dd1f96baeea61df.png)



还有，对于`pipline`来说，每一个item都要经过的，定义多个pipeline之间是有优先级的，数字越小优先级越高，优先级高的pipeline会先处理item，再返回一个item交给下一个pipeline处理；

还有对于`extension`，一般是用来最后生成统计信息的，比如爬取了多少数据，花费了多少时间等；